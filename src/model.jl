module ML

using Random, Statistics
using LinearAlgebra, Flux, Zygote

import Base:
    length, reverse, iterate

export model, train!, validate, preprocess, update_dimension

# NOTE: all data is assumed to be dáµ¢ x N shaped here!
#       this is to allow for fast matrix multiplication through the network
#       it might be worthwhile to keep both memory layouts stored...

# ------------------------------------------------------------------------
# globals

Ïƒâ‚€(x) = x + (sqrt(x^2 +1)-1)/2

# ------------------------------------------------------------------------
# types

# Iterator
struct LayerIterator
    width     :: Array{Int}
    dropout   :: Set{Int}
    normalize :: Set{Int}
    Ïƒáµ¢â‚’       :: Function # activation on input/output layers
    Ïƒ         :: Function # activation on latent layers
end

length(it::LayerIterator)  = length(it.width) + length(it.dropout) + length(it.normalize)
reverse(it::LayerIterator) = LayerIterator(
                                reverse(it.width),
                                Set(length(it.width) - i - 1 for i in it.dropout),
                                Set(length(it.width) - i - 1 for i in it.normalize),
                                it.Ïƒáµ¢â‚’,
                                it.Ïƒ,
                             )

function iterate(it::LayerIterator)
    wâ‚ = it.width[1]
    wâ‚‚ = it.width[2]
    f  = Dense(wâ‚, wâ‚‚, it.Ïƒáµ¢â‚’) |> gpu

    return f, (
        index     = 2,
        dropout   = 1 âˆˆ it.dropout,
        normalize = 1 âˆˆ it.normalize,
    )
end

function iterate(it::LayerIterator, state)
    return if state.dropout
               Dropout(0.5) |> gpu, (
                   index     = state.index,
                   dropout   = false,
                   normalize = state.normalize,
               )
           elseif state.normalize
               BatchNorm(it.width[state.index]) |> gpu, (
                   index     = state.index,
                   dropout   = false,
                   normalize = false,
               )
           elseif state.index < length(it.width)
                wâ‚ = it.width[state.index]
                wâ‚‚ = it.width[state.index+1]

                i  = state.index+1
                f  = Dense(wâ‚, wâ‚‚, i == length(it.width) ? it.Ïƒáµ¢â‚’ : it.Ïƒ) |> gpu

                f, (
                     index     = i,
                     dropout   = (i-1) âˆˆ it.dropout,
                     normalize = (i-1) âˆˆ it.normalize,
                )
           else
               nothing
           end
end

# ------------------------------------------------------------------------
# data scaling / preprocessing

# x is assumed to be dáµ¢ x N shaped
function preprocess(x; dâ‚’::Union{Nothing,Int}=nothing, Ï•=(x)->x)
    X = gpu(x)
	F = svd(X)
	
	d = F.Vt
	Î¼ = mean(d, dims=2)
	Ïƒ = std(d, dims=2)
	
    Î» = Ï•.(F.S)
    Î» = Î» ./ sum(Î»)
	
    if isnothing(dâ‚’)
        dâ‚’ = size(d,1)
    end

	return (
        data   = (d[1:dâ‚’,:] .- Î¼[1:dâ‚’]) ./ Ïƒ[1:dâ‚’], 
        weight = Î»[1:dâ‚’], 
        map    = (x) -> (F.U[:,1:dâ‚’] * Diagonal(F.S[1:dâ‚’])) * ((Ïƒ[1:dâ‚’]) .* x .+ Î¼[1:dâ‚’])
    )
end

# ------------------------------------------------------------------------
# functions

function model(dáµ¢, dâ‚’; Ws=Int[], normalizes=Int[], dropouts=Int[], Ïƒ=elu)
    # check for obvious errors here
    length(dropouts) > 0   && length(Ws) < maximum(dropouts) â‰¤ 0   && error("invalid dropout layer position")
    length(normalizes) > 0 && length(Ws) < maximum(normalizes) â‰¤ 0 && error("invalid normalization layer position")

    layers = LayerIterator(
                    [dáµ¢; Ws; dâ‚’], 
                    Set(dropouts),
                    Set(normalizes), 
                    Ïƒâ‚€, Ïƒ
             )

    F   = Chain(layers...)
    FÂ¯Â¹ = Chain(reverse(layers)...)
    ð•€   = Chain(F, FÂ¯Â¹)

    return (
        pullback=F,
        pushforward=FÂ¯Â¹,
        identity=ð•€
    )
end

function update_dimension(model, dâ‚’; Ïµ = 1e-6)
    F, FÂ¯Â¹, ð•€ = model

    láµ¢ = F[end]
    lâ‚’ = FÂ¯Â¹[1]

    Wáµ¢, báµ¢ = params(láµ¢)
    Wâ‚’, bâ‚’ = params(lâ‚’)

    size(Wáµ¢,1) == dâ‚’ && return nothing
    size(Wáµ¢,1) >  dâ‚’ && error("can not reduce dimensionality of model") 

    Î´  = dâ‚’ - size(Wáµ¢, 1)

    WÌ„áµ¢ = vcat(Wáµ¢, Ïµ*randn(Î´, size(Wáµ¢,2)))
    bÌ„áµ¢ = vcat(báµ¢, Ïµ*randn(Î´))
    lÌ„áµ¢ = Dense(WÌ„áµ¢, bÌ„áµ¢, láµ¢.Ïƒ)

    WÌ„â‚’ = hcat(Wâ‚’, Ïµ*randn(size(Wâ‚’,1), Î´))
    bÌ„â‚’ = bâ‚’
    lÌ„â‚’ = Dense(WÌ„â‚’, bÌ„â‚’, lâ‚’.Ïƒ)

    F   = Chain( (i < length(F) ? f : lÌ„áµ¢ for (i,f) âˆˆ enumerate(F))...)
    FÂ¯Â¹ = Chain( (i > 1 ? f : lÌ„â‚’ for (i,f) âˆˆ enumerate(FÂ¯Â¹))...)
    ð•€   = Chain(F, FÂ¯Â¹)

    return (
        pullback=F,
        pushforward=FÂ¯Â¹,
        identity=ð•€
    )
end

# loss function factories
reconstruction_loss(model, Î©) = (x) -> begin
    xÌ‚ = model.identity(x)
    return sum(Ï‰*mse(x[i,:], xÌ‚[i,:]) for (i,Ï‰) in enumerate(Î©))
end

# data batching
function batch(data, n)
    N = size(data,2)

    lo(i) = (i-1)*n + 1
    hi(i) = min((i)*n, N)

    Î¹ = randperm(N)

    return (data[:,Î¹[lo(i):hi(i)]] for i in 1:ceil(Int, N/n)), 
           (Î¹[lo(i):hi(i)] for i in 1:ceil(Int, N/n))
end

function validate(data, len)
    Î¹ = randperm(size(data,2))
    return (
        valid = data[:,Î¹[1:len]],
        train = data[:,Î¹[len+1:end]],
    ),(
        valid = Î¹[1:len],
        train = Î¹[len+1:end],
    )
end

function noop(epoch) end

# data training
function train!(model, data, loss; B=64, Î·=1e-3, N=100, log=noop)
    Î˜   = params(model.identity)
    opt = ADAM(Î·)

    trainmode!(model.identity)
    for n âˆˆ 1:N
        X, I = batch(data, B)
        for (i,x) âˆˆ zip(I,X)
            E, backpropagate = pullback(Î˜) do
                loss(x, i, false)
            end

            isnan(E) && @goto done

            âˆ‡Î˜ = backpropagate(1f0)
            Flux.Optimise.update!(opt, Î˜, âˆ‡Î˜)
        end

        log(n)
    end
    @label done
    testmode!(model.identity)
end

# ------------------------------------------------------------------------
# tests

end
